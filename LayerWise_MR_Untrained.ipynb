{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "LayerWise_MR_Untrained.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArghyaPal/MR_SOTA/blob/main/LayerWise_MR_Untrained.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVr4XxcbaqmF"
      },
      "source": [
        "# MRI reconstruction from multicoil data using un-trained neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IIFfUHQaqmj"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5sN3Mfvl56l",
        "outputId": "641e12ac-13ad-4dcc-9d28-3065a49cdc0c"
      },
      "source": [
        "### import helpers\n",
        "!git clone https://github.com/MLI-lab/ConvDecoder.git\n",
        "from ConvDecoder.demo_helper.helpers import *"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ConvDecoder' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQ2etZGIaqmq",
        "outputId": "5d710472-9bc3-45a0-8aba-4bc372874043"
      },
      "source": [
        "from __future__ import print_function\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import h5py\n",
        "import time\n",
        "import copy\n",
        "\n",
        "import torch\n",
        "import torch.optim\n",
        "from torch.autograd import Variable\n",
        "from skimage.measure import compare_ssim\n",
        "\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.benchmark = True\n",
        "dtype = torch.cuda.FloatTensor\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "print(\"num GPUs\",torch.cuda.device_count())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num GPUs 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoa3hYYvYw3q"
      },
      "source": [
        "# Loading the data\n",
        "\n",
        "The following cell clones the data from a remote url. The data consists of a single validation image from the [fastMRI dataset](https://arxiv.org/abs/1811.08839)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4esck6wXprG"
      },
      "source": [
        "import urllib.request as urll\n",
        "proxy_handler = urll.ProxyHandler({})\n",
        "opener = urll.build_opener(proxy_handler)\n",
        "urll.install_opener(opener)\n",
        "data_url =  'https://rice.box.com/shared/static/y1tcaa0eo62ie3lszrkamqivdvirx1x3.h5'\n",
        "req = urll.Request(data_url)\n",
        "r = opener.open(req)\n",
        "result = r.read()\n",
        "\n",
        "### store the downloaded data in the session directory\n",
        "filename = 'file1339.h5'\n",
        "with open(filename, 'wb') as f:\n",
        "  f.write(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjTQGuLMm8Nb"
      },
      "source": [
        "The data contains an entire volume of data, each volume consists of slices. We reconstruct the middle slice, which is selected in the following, and plotted:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQzHzGHxaqm3"
      },
      "source": [
        "### load the data\n",
        "f = h5py.File(filename, 'r') # contains a kspace measurement f['kspace'] and rss reconstruction f['reconstruction_rss']\n",
        "\n",
        "\n",
        "# The f has 3 columns\n",
        "# ['ismrmrd_header', 'kspace', 'reconstruction_rss']\n",
        "# kspace: (33, 15, 640, 368)                         <-- 33 batch size, 15 receiving coils\n",
        "# reconstruction_rss: (33, 320, 320)                 <-- 33 is the batch size\n",
        "\n",
        "\n",
        "## K space data\n",
        "slicenu = f[\"kspace\"].shape[0]//2                   # 33/2 = 16, shape[0] is 33\n",
        "slice_ksp = f['kspace'][slicenu]                    # 15, 640, 368 <-- 15 receiving coils\n",
        "# Convert complex numpy to torch tensor\n",
        "data = slice_ksp.copy()\n",
        "data = np.stack((data.real, data.imag), axis=-1)    # 15, 640, 368, 2 <-- 2 is for the real and imaginary \n",
        "\n",
        "# I am artificially making it single receeiver coil\n",
        "slice_ksp_torchtensor = torch.from_numpy(data)\n",
        "\n",
        "\n",
        "# Corresponding MR Image\n",
        "orig = f[\"reconstruction_rss\"][slicenu]             # 320, 320\n",
        "plt.imshow(orig,'gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkiBmcuzaqnq"
      },
      "source": [
        "<a name='funcs'></a>\n",
        "# Creating the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIN3JA3XSokn"
      },
      "source": [
        "### Network Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SFODuRpY2bh"
      },
      "source": [
        "class layer1(nn.Module):\n",
        "    def __init__(self, num_layers, num_channels, num_output_channels, out_size, in_size):\n",
        "                     # num_layers=7,num_channels=256, output_depth=30, out_size=[640, 368], in_size=[8,4]\n",
        "        super(conv_model, self).__init__()\n",
        "\n",
        "        ### parameter setup\n",
        "        kernel_size = 3\n",
        "        strides = [1]*(num_layers-1)\n",
        "        \n",
        "        ### compute up-sampling factor from one layer to another\n",
        "        scale_x,scale_y = (out_size[0]/in_size[0])**(1./(num_layers-1)), (out_size[1]/in_size[1])**(1./(num_layers-1))\n",
        "        hidden_size = [(int(np.ceil(scale_x**n * in_size[0])),\n",
        "                        int(np.ceil(scale_y**n * in_size[1]))) for n in range(1, (num_layers-1))] + [out_size]\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(num_channels, num_channels, kernel_size, strides[0], padding=(kernel_size-1)//2, bias=True)\n",
        "        self.conv7 = nn.Conv2d(num_channels, num_output_channels, 1, 1, padding=0, bias=True)\n",
        "        self.relu  = nn.ReLU()\n",
        "        self.batch = nn.BatchNorm2d(num_channels, affine=True)\n",
        "\n",
        "\n",
        "    def forward(self, x, scale_out=1):\n",
        "            layer_1 = self.batch(self.relu(self.conv1(x)))\n",
        "            x = self.conv7(layer_1)\n",
        "            return layer_2, x*scale_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRTiOPkN3mm9"
      },
      "source": [
        "class layer2(nn.Module):\n",
        "    def __init__(self, num_layers, num_channels, num_output_channels, out_size, in_size):\n",
        "                     # num_layers=7,num_channels=256, output_depth=30, out_size=[640, 368], in_size=[8,4]\n",
        "        super(conv_model, self).__init__()\n",
        "\n",
        "        ### parameter setup\n",
        "        kernel_size = 3\n",
        "        strides = [1]*(num_layers-1)\n",
        "        \n",
        "        ### compute up-sampling factor from one layer to another\n",
        "        scale_x,scale_y = (out_size[0]/in_size[0])**(1./(num_layers-1)), (out_size[1]/in_size[1])**(1./(num_layers-1))\n",
        "        hidden_size = [(int(np.ceil(scale_x**n * in_size[0])),\n",
        "                        int(np.ceil(scale_y**n * in_size[1]))) for n in range(1, (num_layers-1))] + [out_size]\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(num_channels, num_channels, kernel_size, strides[0], padding=(kernel_size-1)//2, bias=True)\n",
        "        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size, strides[1], padding=(kernel_size-1)//2, bias=True)\n",
        "        self.relu  = nn.ReLU()\n",
        "        self.batch = nn.BatchNorm2d(num_channels, affine=True)\n",
        "\n",
        "\n",
        "    def forward(self, x, scale_out=1):\n",
        "            layer_1 = self.batch(self.relu(self.conv1(x)))\n",
        "            x = self.batch(self.relu(self.conv1(layer_1)))\n",
        "            return layer_1, x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueN_EJcuVhSY"
      },
      "source": [
        "### Scaling\n",
        "\n",
        "The target of the decoder (i.e., the under-sampled measurement) needs to be scaled w.r.t to the output range of the network in order for the network to work well. We do that by simply multiplying the measurement by a scaling factor computed as $\\frac{\\lVert \\text{network output} \\lVert_2}{\\lVert \\text{zero-padded image} \\rVert_2}$, where the zero-padded image is obtained by applying the inverse Fourier trnsform to the under-sampled measurement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEIXNnQeUoCt"
      },
      "source": [
        "def get_scale_factor(net,num_channels,in_size,masked_kspace,ni=None): \n",
        "    ### get norm of deep decoder output\n",
        "    # get net input, scaling of that is irrelevant\n",
        "    if ni is None:\n",
        "        shape = [1,num_channels, in_size[0], in_size[1]]\n",
        "        ni = Variable(torch.zeros(shape)).type(dtype)\n",
        "        ni.data.uniform_()\n",
        "    # generate random image for the above net input\n",
        "    out_chs = net( ni.type(dtype) ).data.cpu().numpy()[0]\n",
        "    out_imgs = channels2imgs(out_chs)\n",
        "    out_img_tt = root_sum_of_squares( torch.tensor(out_imgs) , dim=0)\n",
        "\n",
        "    ### get norm of zero-padded image\n",
        "    orig_tt = ifft2(masked_kspace)        # Apply Inverse Fourier Transform to get the complex image\n",
        "    orig_imgs_tt = complex_abs(orig_tt)   # Compute absolute value to get a real image\n",
        "    orig_img_tt = root_sum_of_squares(orig_imgs_tt, dim=0)\n",
        "    orig_img_np = orig_img_tt.cpu().numpy()\n",
        "    \n",
        "    ### compute scaling factor as norm(output)/norm(ground truth)\n",
        "    s = np.linalg.norm(out_img_tt) / np.linalg.norm(orig_img_np)\n",
        "    return s,ni"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulesD3Y1x3tR"
      },
      "source": [
        "### Instanciating a network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4_5tgDzaqnb"
      },
      "source": [
        "### Hyper-parameters\n",
        "num_iters = 2500       # number of gradient iterations\n",
        "in_size = [8,4]        # network input size\n",
        "kernel_size = 3        # size of convolutional filters\n",
        "num_layers = 7         # number of layers\n",
        "num_channels = 256     # number of channels per layer (except the final layer)\n",
        "output_depth = slice_ksp_torchtensor.numpy().shape[0]*2 # number of output channels\n",
        "out_size = slice_ksp_torchtensor.numpy().shape[1:-1]    # network output size\n",
        "\n",
        "### create a randomly-initialized network ###\n",
        "net = layer1(num_layers,num_channels, output_depth,out_size,in_size).type(dtype)\n",
        "\n",
        "net_1 = layer1(num_layers,num_channels, output_depth,out_size,in_size).type(dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9EZd8jtyIvB"
      },
      "source": [
        "### Initializing the network\n",
        "\n",
        "The network instanciated above is un-trained. We can work with it, without any further modifications. However, initializing it by fitting it to a single training example improves the reconstruction speed by a factor of about 10, but doesn't improve reconstruction accuray. To speed up reconstruction, we therefore load an initialization from a file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kZmw4Qkr3XO"
      },
      "source": [
        "### load the initialization\n",
        "state_url = 'https://rice.box.com/shared/static/hp8o7d1gnd4gux203vubzccuefq03qmi'\n",
        "checkpoint = torch.hub.load_state_dict_from_url(state_url, progress=True)\n",
        "\n",
        "\n",
        "ni = Variable(torch.zeros((1, 256, 640, 368))).type(dtype) #checkpoint['net_input'].type(dtype)\n",
        "\n",
        "# the masks are loaded from data; alternatively we can load them from the checkpoint with:\n",
        "#mask, mask1d, mask2d = get_mask(slice_ksp_torchtensor, slice_ksp)\n",
        "\n",
        "mask = checkpoint['mask']; mask1d = checkpoint['mask1d']; mask2d = checkpoint['mask2d'];\n",
        "\n",
        "#print(mask2d.shape)\n",
        "#print(ni.shape)\n",
        "plt.imshow(mask2d,'gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRwWLtJJnJji"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbsG1lQyPD4-"
      },
      "source": [
        "### fix scaling for the decoder\n",
        "masked_kspace, _ = apply_mask(slice_ksp_torchtensor, mask = mask)\n",
        "scaling_factor,ni = get_scale_factor(net,\n",
        "                                     num_channels,\n",
        "                                     in_size,\n",
        "                                     masked_kspace,\n",
        "                                     ni = ni,)\n",
        "masked_kspace *= scaling_factor\n",
        "unders_measurement = Variable(masked_kspace[None,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6dSwHMCaqnS"
      },
      "source": [
        "# Fitting the un-trained network to the measurements\n",
        "We fit the network's parameters so that its output passed through the forward model describes the measurements well. For this, we define the forward *model* and fit function:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oIULsPG61sY"
      },
      "source": [
        "### Forward model\n",
        "\n",
        "The forward process applies the Fourier transform followed by an under-sampling mask."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXVSZUra6z1X"
      },
      "source": [
        "def forwardm(img,mask):\n",
        "    # img has dimension (2*num_slices, x,y)\n",
        "    # output has dimension (1, num_slices, x, y, 2)\n",
        "    mask = np_to_var(mask)[0].type(dtype)\n",
        "    s = img.shape\n",
        "    ns = int(s[1]/2) # number of slices\n",
        "    fimg = Variable( torch.zeros( (s[0],ns,s[2],s[3],2 ) ) ).type(dtype)\n",
        "    ### re-shaping image coils\n",
        "    for i in range(ns):\n",
        "        fimg[0,i,:,:,0] = img[0,2*i,:,:]\n",
        "        fimg[0,i,:,:,1] = img[0,2*i+1,:,:]\n",
        "    ### apply 2D Fourier transform\n",
        "    Fimg = fft2(fimg) # dim: (1,num_slices,x,y,2)\n",
        "    ### apply the mask\n",
        "    for i in range(ns):\n",
        "        Fimg[0,i,:,:,0] *= mask\n",
        "        Fimg[0,i,:,:,1] *= mask\n",
        "    return Fimg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D19mZq5wT4S3"
      },
      "source": [
        "### Optimization\n",
        "\n",
        "The following function performs the optimization by updating the network weights with respect to the MSE loss function and the given under-sampled measurements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEMM81egaqoB"
      },
      "source": [
        "def fit(net,              # network\n",
        "        img_noisy_var,    # under-sampled measurement\n",
        "        num_channels,     # number of channles in each layer (it's a list containing #channels for all layers)\n",
        "        net_input,        # network input\n",
        "        apply_f,          # forwrd model\n",
        "        mask,             # under-sampling mask\n",
        "        num_iter = 5000,  # number of gradient iterations\n",
        "        LR = 0.01,        # stepsize\n",
        "       ):\n",
        "    \n",
        "    net_input = net_input.type(dtype)\n",
        "    p = [x for x in net.parameters() ]\n",
        "\n",
        "    mse_wrt_noisy = np.zeros(num_iter)\n",
        "    \n",
        "    ### optimizer\n",
        "    print(\"optimize with adam\", LR)\n",
        "    optimizer = torch.optim.Adam(p, lr=LR)\n",
        "\n",
        "    ### loss function\n",
        "    mse = torch.nn.MSELoss() \n",
        "    \n",
        "    ### we return the best network across the iterations w.r.t fitting loss\n",
        "    best_net = copy.deepcopy(net)\n",
        "    best_mse = 1000000.0\n",
        "        \n",
        "    ### main optimization loop\n",
        "    for i in range(num_iter):\n",
        "        \n",
        "        def closure():\n",
        "            optimizer.zero_grad()\n",
        "            _, out = net(net_input.type(dtype))\n",
        "\n",
        "            \n",
        "            # training loss \n",
        "            loss = mse( apply_f(out,mask) , img_noisy_var )\n",
        "            loss.backward()\n",
        "            mse_wrt_noisy[i] = loss.data.cpu().numpy()\n",
        "                  \n",
        "            ### display the fitting loss every 10 iterations\n",
        "            if i % 10 == 0:\n",
        "                print ('Iteration %05d    Train loss %f ' % (i, loss.data), '\\r', end='')\n",
        "            return loss   \n",
        "        ### call the optimization step\n",
        "        loss = optimizer.step(closure)\n",
        "        # if training loss improves by at least one percent, we found a new best net\n",
        "        if best_mse > 1.005*loss.data:\n",
        "            best_mse = loss.data\n",
        "            best_net = copy.deepcopy(net)\n",
        "    \n",
        "\n",
        "    ### main optimization loop\n",
        "    for i in range(num_iter):\n",
        "        \n",
        "        def closure():\n",
        "            optimizer.zero_grad()\n",
        "            inter, _ = net(net_input.type(dtype))\n",
        "            _, otpt = net_1(inter)\n",
        "            # training loss \n",
        "            loss = mse( inter, otpt )\n",
        "            loss.backward()\n",
        "            mse_wrt_noisy[i] = loss.data.cpu().numpy()\n",
        "                  \n",
        "            ### display the fitting loss every 10 iterations\n",
        "            if i % 10 == 0:\n",
        "                print ('Iteration %05d    Train loss %f ' % (i, loss.data), '\\r', end='')\n",
        "            return loss   \n",
        "        ### call the optimization step\n",
        "        loss = optimizer.step(closure)\n",
        "        # if training loss improves by at least one percent, we found a new best net\n",
        "        if best_mse > 1.005*loss.data:\n",
        "            best_mse = loss.data\n",
        "            best_net = copy.deepcopy(net)\n",
        "                 \n",
        "    net = best_net\n",
        "    return mse_wrt_noisy, net "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4wGLEjcWWkz"
      },
      "source": [
        "### Data consistency\n",
        "\n",
        "Data consistency is the last step of the reconstruction. After we fit the network and get the reconstructed image, we enforce consistency by taking the frequency elements we know from the under-sampled measurement and put them in the Fourier representation of the reconstruction.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlmCqErHWZ0a"
      },
      "source": [
        "def data_consistency(net, ni, mask1d, slice_ksp_torchtensor1):    \n",
        "    img = net(ni.type(dtype))\n",
        "    s = img.shape\n",
        "    ns = int(s[1]/2) # number of slices\n",
        "    fimg = Variable( torch.zeros( (s[0],ns,s[2],s[3],2 ) ) ).type(dtype)\n",
        "    ### get under-sampled measurement and put it in the Fourier representation of the reconstruction\n",
        "    for i in range(ns):\n",
        "        fimg[0,i,:,:,0] = img[0,2*i,:,:]\n",
        "        fimg[0,i,:,:,1] = img[0,2*i+1,:,:]\n",
        "    Fimg = fft2(fimg) # dim: (1,num_slices,x,y,2)\n",
        "    # ksp has dim: (num_slices,x,y)\n",
        "    meas = slice_ksp_torchtensor1.unsqueeze(0).data.cpu() # dim: (1,num_slices,x,y,2)\n",
        "    mask = torch.from_numpy(np.array(mask1d, dtype=np.uint8))\n",
        "    ksp_dc = Fimg.clone()\n",
        "    ksp_dc = ksp_dc.detach().cpu()\n",
        "    ksp_dc[:,:,:,mask==1,:] = meas[:,:,:,mask==1,:] # after data consistency block\n",
        "\n",
        "    ### compute the inverse fourier transform of the consistency-enforced k-space\n",
        "    img_dc = ifft2(ksp_dc)[0]\n",
        "    out = []\n",
        "    for img in img_dc.detach().cpu():\n",
        "        out += [ img[:,:,0].numpy() , img[:,:,1].numpy() ]\n",
        "\n",
        "    ### apply root sum of squares and then crop the image\n",
        "    par_out_chs = np.array(out)\n",
        "    par_out_imgs = channels2imgs(par_out_chs)\n",
        "    prec = root_sum_of_squares(torch.from_numpy(par_out_imgs)).numpy()\n",
        "    if prec.shape[0] > 320:\n",
        "        prec = crop_center(prec,320,320)\n",
        "    return prec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-n1YcjhVzzcm"
      },
      "source": [
        "### Finally - Image recovery\n",
        "\n",
        "We are now ready to reconstruct the image with the un-trained neural network - this runs for about 15 minutes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr0dU1CWaqni"
      },
      "source": [
        "### reconstruct ###\n",
        "start = time.time()\n",
        "\n",
        "mse_wrt_noisy, net  = fit(copy.deepcopy(net),\n",
        "                          unders_measurement.type(dtype),\n",
        "                          [num_channels]*(num_layers-1),\n",
        "                          net_input = ni,\n",
        "                          apply_f = forwardm,\n",
        "                          mask = mask2d,\n",
        "                          num_iter=num_iters,\n",
        "                          LR=0.008,\n",
        "                          )\n",
        "\n",
        "### apply data consistency to finalize the reconstruction\n",
        "rec = data_consistency(net, ni, mask1d, scaling_factor*slice_ksp_torchtensor.data.cpu())\n",
        "\n",
        "print('\\nfinished after %.1f minutes.'%((time.time()-start)/60))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-svO_XtURxl"
      },
      "source": [
        "# Evaluating the reconstruction\n",
        "\n",
        "Finally we evaluate our reconstruction. \n",
        "\n",
        "The following functions are for normalizing the output and computing the SSIM score. As shown in our paper, the choice of normalization affects the score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKjrM0NTaqnt"
      },
      "source": [
        "def normalize(im1,im2):\n",
        "    # im1: ground truth\n",
        "    # im2: reconstruction\n",
        "    im1 = (im1-im1.mean()) / im1.std()\n",
        "    im1 *= im2.std()\n",
        "    im1 += im2.mean()\n",
        "    return im1,im2\n",
        "def ssim(gt, pred):\n",
        "    \"\"\" Compute Structural Similarity Index Metric (SSIM). \"\"\"\n",
        "    return compare_ssim(\n",
        "        gt.transpose(1, 2, 0), pred.transpose(1, 2, 0), multichannel=True, data_range=gt.max()\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZUjnG916NlL"
      },
      "source": [
        "### evaluate the reconstruction quality\n",
        "im1,im2 = normalize(orig,rec)\n",
        "score = ssim(np.array([im1]),np.array([im2]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHwpnnRpaqnm"
      },
      "source": [
        "### Visualize the output ###\n",
        "fig = plt.figure(figsize=(12,5))\n",
        "ax = fig.add_subplot(121)\n",
        "ax.imshow(orig,'gray')\n",
        "ax.set_title(filename.split('/')[-1])\n",
        "ax.axis('off')\n",
        "\n",
        "ax = fig.add_subplot(122)\n",
        "ax.imshow(rec,'gray')\n",
        "ax.set_title('SSIM: %.4f'%(score))\n",
        "ax.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSDdaVEEhK2z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}